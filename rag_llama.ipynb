{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, RagRetriever, RagSequenceForGeneration\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d : %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Spacy model for POS and DEP tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for POS and DEP tagging\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    dep_tags = [token.dep_ for token in doc]\n",
    "    return tokens, pos_tags, dep_tags\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_and_preprocess_dataset(dataset_name=\"squad\", split=\"train[:10%]\"):\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    preprocessed_data = []\n",
    "    \n",
    "    for entry in tqdm(dataset):\n",
    "        question = entry['question']\n",
    "        context = entry['context']\n",
    "        \n",
    "        # POS and DEP tagging\n",
    "        context_tokens, context_pos, context_dep = preprocess_text(context)\n",
    "        question_tokens, question_pos, question_dep = preprocess_text(question)\n",
    "        \n",
    "        preprocessed_data.append({\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"context_tokens\": context_tokens,\n",
    "            \"context_pos\": context_pos,\n",
    "            \"context_dep\": context_dep,\n",
    "            \"question_tokens\": question_tokens,\n",
    "            \"question_pos\": question_pos,\n",
    "            \"question_dep\": question_dep,\n",
    "        })\n",
    "    \n",
    "    return preprocessed_data\n",
    "\n",
    "# Load JSON files and extract documents\n",
    "def load_json_files(dataset_dir):\n",
    "    json_files = [\n",
    "        os.path.join(dataset_dir, os.getenv('DEP_MAPPING_FILE')),\n",
    "        os.path.join(dataset_dir, os.getenv('POS_MAPPING_FILE')),\n",
    "        os.path.join(dataset_dir, os.getenv('TEST_FILE')),\n",
    "        os.path.join(dataset_dir, os.getenv('TRAIN_FILE')),\n",
    "        os.path.join(dataset_dir, os.getenv('VAL_FILE'))\n",
    "    ]\n",
    "\n",
    "    documents = []\n",
    "    for file in json_files:\n",
    "        if not os.path.exists(file):\n",
    "            print(f\"Warning: {file} not found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        with open(file, 'r') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, dict):\n",
    "                    data = [data]\n",
    "\n",
    "                for item in data:\n",
    "                    if 'question' in item:\n",
    "                        document = {\n",
    "                            'page_content': item['question'],\n",
    "                            'metadata': {\n",
    "                                'pos_tags': item.get('question_pos_tokens', []),\n",
    "                                'dep_tags': item.get('question_dep_ids', []),\n",
    "                            }\n",
    "                        }\n",
    "                        documents.append(Document(**document))\n",
    "                    else:\n",
    "                        print(f\"Warning: No suitable field found in {file}. Skipping this item.\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error: Could not decode JSON from {file}. Skipping.\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to your datasets\n",
    "dataset_dirs = [\n",
    "    os.getenv('DATASET_DIR_1'), \n",
    "    os.getenv('DATASET_DIR_2'), \n",
    "    os.getenv('DATASET_DIR_3')\n",
    "]\n",
    "all_documents = []\n",
    "\n",
    "# Loop through each dataset and load all the documents\n",
    "for dataset_dir in dataset_dirs:\n",
    "    documents = load_json_files(dataset_dir)\n",
    "    all_documents.extend(documents)\n",
    "\n",
    "print(f\"Total documents loaded: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings and vector store\n",
    "embed_model_id = 'multi-qa-distilbert-cos-v1'\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "core_embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id\n",
    ")\n",
    "\n",
    "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    core_embeddings_model, store, namespace=embed_model_id\n",
    ")\n",
    "\n",
    "vector_store = FAISS.from_documents(all_documents, embedder)\n",
    "vector_store.save_local('./vector_store/rag_task_vector_store')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLAMA model\n",
    "model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "generate_text = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up RetrievalQA with the vector store and LLAMA model\n",
    "def ask_questions_from_df(df_questions, vector_store, llm):\n",
    "    df_questions['Answer'] = None\n",
    "    handler = StdOutCallbackHandler()\n",
    "\n",
    "    for index, row in df_questions.iterrows():\n",
    "        question = row['Question']\n",
    "        response = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            retriever=vector_store.as_retriever(search_kwargs={\"k\": 5}),\n",
    "            callbacks=[handler],\n",
    "            return_source_documents=True\n",
    "        )({\"query\": question})\n",
    "        \n",
    "        answer = response['result']\n",
    "        df_questions.at[index, 'Answer'] = answer\n",
    "\n",
    "    return df_questions\n",
    "\n",
    "# Example of asking questions (replace with your own dataframe)\n",
    "# df_questions = pd.read_csv(\"path_to_your_questions.csv\")\n",
    "# df_questions_with_answers = ask_questions_from_df(df_questions, vector_store, llm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
