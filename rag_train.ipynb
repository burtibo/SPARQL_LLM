{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    RagTokenizer, \n",
    "    RagRetriever, \n",
    "    RagSequenceForGeneration\n",
    ")\n",
    "import logging, json, spacy\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d : %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load Spacy model for POS and DEP tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for POS and DEP tagging\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    dep_tags = [token.dep_ for token in doc]\n",
    "    return tokens, pos_tags, dep_tags\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_and_preprocess_dataset(dataset_name=\"squad\", split=\"train[:10%]\"):\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    preprocessed_data = []\n",
    "    \n",
    "    for entry in tqdm(dataset):\n",
    "        question = entry['question']\n",
    "        context = entry['context']\n",
    "        \n",
    "        # POS and DEP tagging\n",
    "        context_tokens, context_pos, context_dep = preprocess_text(context)\n",
    "        question_tokens, question_pos, question_dep = preprocess_text(question)\n",
    "        \n",
    "        # Tokenize and encode the inputs\n",
    "        input_ids = tokenizer(question, context, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "        \n",
    "        preprocessed_data.append({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"context_tokens\": context_tokens,\n",
    "            \"context_pos\": context_pos,\n",
    "            \"context_dep\": context_dep,\n",
    "            \"question_tokens\": question_tokens,\n",
    "            \"question_pos\": question_pos,\n",
    "            \"question_dep\": question_dep,\n",
    "        })\n",
    "    \n",
    "    return Dataset.from_dict(preprocessed_data)\n",
    "\n",
    "# Load JSON files and extract documents\n",
    "def load_json_files(dataset_dir):\n",
    "    json_files = [\n",
    "        os.path.join(dataset_dir, os.getenv('DEP_MAPPING_FILE')),\n",
    "        os.path.join(dataset_dir, os.getenv('POS_MAPPING_FILE')),\n",
    "        os.path.join(dataset_dir, os.getenv('TEST_FILE')),\n",
    "        os.path.join(dataset_dir, os.getenv('TRAIN_FILE')),\n",
    "        os.path.join(dataset_dir, os.getenv('VAL_FILE'))\n",
    "    ]\n",
    "\n",
    "    documents = []\n",
    "    for file in json_files:\n",
    "        if not os.path.exists(file):\n",
    "            print(f\"Warning: {file} not found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        with open(file, 'r') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, dict):\n",
    "                    data = [data]\n",
    "\n",
    "                for item in data:\n",
    "                    if 'question' in item:\n",
    "                        document = {\n",
    "                            'text': item['question'],\n",
    "                            'pos_tags': item.get('question_pos_tokens', []),\n",
    "                            'dep_tags': item.get('question_dep_ids', [])\n",
    "                        }\n",
    "                        documents.append(document)\n",
    "                    else:\n",
    "                        print(f\"Warning: No suitable field found in {file}. Skipping this item.\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error: Could not decode JSON from {file}. Skipping.\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Loop through each dataset and load all the documents\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_dir \u001b[38;5;129;01min\u001b[39;00m dataset_dirs:\n\u001b[1;32m---> 11\u001b[0m     documents \u001b[38;5;241m=\u001b[39m \u001b[43mload_json_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     all_documents\u001b[38;5;241m.\u001b[39mextend(documents)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal documents loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 40\u001b[0m, in \u001b[0;36mload_json_files\u001b[1;34m(dataset_dir)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json_files\u001b[39m(dataset_dir):\n\u001b[0;32m     39\u001b[0m     json_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 40\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDEP_MAPPING_FILE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     41\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_dir, os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOS_MAPPING_FILE\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     42\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_dir, os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEST_FILE\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     43\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_dir, os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAIN_FILE\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     44\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_dir, os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAL_FILE\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     45\u001b[0m     ]\n\u001b[0;32m     47\u001b[0m     documents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m json_files:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ntpath.py:78\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(path, \u001b[38;5;241m*\u001b[39mpaths):\n\u001b[1;32m---> 78\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m     80\u001b[0m         sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# Define paths to your datasets\n",
    "dataset_dirs = [\n",
    "    os.getenv('DATASET_DIR_1'), \n",
    "    os.getenv('DATASET_DIR_2'), \n",
    "    os.getenv('DATASET_DIR_3')\n",
    "]\n",
    "all_documents = []\n",
    "\n",
    "# Loop through each dataset and load all the documents\n",
    "for dataset_dir in dataset_dirs:\n",
    "    documents = load_json_files(dataset_dir)\n",
    "    all_documents.extend(documents)\n",
    "\n",
    "print(f\"Total documents loaded: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No suitable field found in data/lcquad2\\dep_mapping.json. Skipping this item.\n",
      "Warning: No suitable field found in data/lcquad2\\pos_mapping.json. Skipping this item.\n",
      "Warning: No suitable field found in data/qald9\\dep_mapping.json. Skipping this item.\n",
      "Warning: No suitable field found in data/qald9\\pos_mapping.json. Skipping this item.\n",
      "Warning: No suitable field found in data/vquanda\\dep_mapping.json. Skipping this item.\n",
      "Warning: No suitable field found in data/vquanda\\pos_mapping.json. Skipping this item.\n",
      "Total documents loaded: 35413\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained RAG model and tokenizer\n",
    "rag_tokenizer = RagTokenizer.from_pretrained(os.getenv('RAG_MODEL_NAME'))\n",
    "rag_model = RagSequenceForGeneration.from_pretrained(os.getenv('RAG_MODEL_NAME'))\n",
    "\n",
    "# Initialize the retriever with the combined documents\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    os.getenv('RAG_MODEL_NAME'),\n",
    "    index_name=\"custom\",\n",
    "    passages=all_documents  \n",
    ")\n",
    "\n",
    "# Save the retriever for later use\n",
    "retriever.save_pretrained(\"retriever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 35413/35413 [00:00<00:00, 2796544.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=os.getenv('OUTPUT_DIR'),\n",
    "    evaluation_strategy=os.getenv('EVAL_STRATEGY'),\n",
    "    learning_rate=float(os.getenv('LEARNING_RATE')),\n",
    "    per_device_train_batch_size=int(os.getenv('TRAIN_BATCH_SIZE')),\n",
    "    per_device_eval_batch_size=int(os.getenv('EVAL_BATCH_SIZE')),\n",
    "    weight_decay=float(os.getenv('WEIGHT_DECAY')),\n",
    "    save_total_limit=int(os.getenv('SAVE_TOTAL_LIMIT')),\n",
    "    num_train_epochs=int(os.getenv('NUM_TRAIN_EPOCHS')),\n",
    "    predict_with_generate=bool(os.getenv('PREDICT_WITH_GENERATE')),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "Map: 100%|██████████| 35413/35413 [00:04<00:00, 7717.39 examples/s] \n",
      "  3%|▎         | 1/36 [00:00<00:00, 125.04it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Add a Faiss index to the dataset\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_faiss_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Define the path where the index will be saved\u001b[39;00m\n\u001b[0;32m     18\u001b[0m index_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/index\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\31615\\Desktop\\thesis\\SPARQL_LLM\\thesis\\lib\\site-packages\\datasets\\arrow_dataset.py:5936\u001b[0m, in \u001b[0;36mDataset.add_faiss_index\u001b[1;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose, dtype)\u001b[0m\n\u001b[0;32m   5882\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add a dense index using Faiss for fast retrieval.\u001b[39;00m\n\u001b[0;32m   5883\u001b[0m \u001b[38;5;124;03mBy default the index is done over the vectors of the specified column.\u001b[39;00m\n\u001b[0;32m   5884\u001b[0m \u001b[38;5;124;03mYou can specify `device` if you want to run it on GPU (`device` must be the GPU index).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5933\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m   5934\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5935\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatted_as(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[column], dtype\u001b[38;5;241m=\u001b[39mdtype):\n\u001b[1;32m-> 5936\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_faiss_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstring_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstring_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaiss_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfaiss_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\31615\\Desktop\\thesis\\SPARQL_LLM\\thesis\\lib\\site-packages\\datasets\\search.py:490\u001b[0m, in \u001b[0;36mIndexableMixin.add_faiss_index\u001b[1;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose)\u001b[0m\n\u001b[0;32m    486\u001b[0m index_name \u001b[38;5;241m=\u001b[39m index_name \u001b[38;5;28;01mif\u001b[39;00m index_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[0;32m    487\u001b[0m faiss_index \u001b[38;5;241m=\u001b[39m FaissIndex(\n\u001b[0;32m    488\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice, string_factory\u001b[38;5;241m=\u001b[39mstring_factory, metric_type\u001b[38;5;241m=\u001b[39mmetric_type, custom_index\u001b[38;5;241m=\u001b[39mcustom_index\n\u001b[0;32m    489\u001b[0m )\n\u001b[1;32m--> 490\u001b[0m \u001b[43mfaiss_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_vectors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaiss_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfaiss_verbose\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indexes[index_name] \u001b[38;5;241m=\u001b[39m faiss_index\n",
      "File \u001b[1;32mc:\\Users\\31615\\Desktop\\thesis\\SPARQL_LLM\\thesis\\lib\\site-packages\\datasets\\search.py:313\u001b[0m, in \u001b[0;36mFaissIndex.add_vectors\u001b[1;34m(self, vectors, column, batch_size, train_size, faiss_verbose)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(vectors), batch_size)):\n\u001b[0;32m    312\u001b[0m     vecs \u001b[38;5;241m=\u001b[39m vectors[i : i \u001b[38;5;241m+\u001b[39m batch_size] \u001b[38;5;28;01mif\u001b[39;00m column \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m vectors[i : i \u001b[38;5;241m+\u001b[39m batch_size][column]\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfaiss_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvecs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\31615\\Desktop\\thesis\\SPARQL_LLM\\thesis\\lib\\site-packages\\faiss\\class_wrappers.py:228\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_add\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Adds vectors to the index.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03mThe index must be trained before vectors can be added to it.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03mThe vectors are implicitly numbered in sequence. When `n` vectors are\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    `dtype` must be float32.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    227\u001b[0m n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n\u001b[0;32m    229\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_c(n, swig_ptr(x))\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset\n",
    "dataset = load_and_preprocess_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=rag_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
