{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    RagTokenizer, \n",
    "    RagRetriever, \n",
    "    RagSequenceForGeneration\n",
    ")\n",
    "import logging, json, spacy\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d : %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Spacy model for POS and DEP tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for POS and DEP tagging\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    dep_tags = [token.dep_ for token in doc]\n",
    "    return tokens, pos_tags, dep_tags\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_and_preprocess_dataset(dataset_name=\"squad\", split=\"train[:10%]\"):\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    preprocessed_data = []\n",
    "    \n",
    "    for entry in tqdm(dataset):\n",
    "        question = entry['question']\n",
    "        context = entry['context']\n",
    "        \n",
    "        # POS and DEP tagging\n",
    "        context_tokens, context_pos, context_dep = preprocess_text(context)\n",
    "        question_tokens, question_pos, question_dep = preprocess_text(question)\n",
    "        \n",
    "        # Tokenize and encode the inputs\n",
    "        input_ids = tokenizer(question, context, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "        \n",
    "        preprocessed_data.append({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"context_tokens\": context_tokens,\n",
    "            \"context_pos\": context_pos,\n",
    "            \"context_dep\": context_dep,\n",
    "            \"question_tokens\": question_tokens,\n",
    "            \"question_pos\": question_pos,\n",
    "            \"question_dep\": question_dep,\n",
    "        })\n",
    "    \n",
    "    return Dataset.from_dict(preprocessed_data)\n",
    "\n",
    "# Load JSON files and extract documents\n",
    "def load_json_files(dataset_dir):\n",
    "    json_files = [\n",
    "        os.path.join(dataset_dir, os.getenv('DEP_MAPPING_FILE')),\n",
    "        os.path.join(dataset_dir, os.getenv('POS_MAPPING_FILE')),\n",
    "        os.path.join(dataset_dir, os.getenv('TEST_FILE')),\n",
    "        os.path.join(dataset_dir, os.getenv('TRAIN_FILE')),\n",
    "        os.path.join(dataset_dir, os.getenv('VAL_FILE'))\n",
    "    ]\n",
    "\n",
    "    documents = []\n",
    "    for file in json_files:\n",
    "        if not os.path.exists(file):\n",
    "            print(f\"Warning: {file} not found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        with open(file, 'r') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, dict):\n",
    "                    data = [data]\n",
    "\n",
    "                for item in data:\n",
    "                    if 'question' in item:\n",
    "                        document = {\n",
    "                            'text': item['question'],\n",
    "                            'pos_tags': item.get('question_pos_tokens', []),\n",
    "                            'dep_tags': item.get('question_dep_ids', [])\n",
    "                        }\n",
    "                        documents.append(document)\n",
    "                    else:\n",
    "                        print(f\"Warning: No suitable field found in {file}. Skipping this item.\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error: Could not decode JSON from {file}. Skipping.\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to your datasets\n",
    "dataset_dirs = [\n",
    "    os.getenv('DATASET_DIR_1'), \n",
    "    os.getenv('DATASET_DIR_2'), \n",
    "    os.getenv('DATASET_DIR_3')\n",
    "]\n",
    "all_documents = []\n",
    "\n",
    "# Loop through each dataset and load all the documents\n",
    "for dataset_dir in dataset_dirs:\n",
    "    documents = load_json_files(dataset_dir)\n",
    "    all_documents.extend(documents)\n",
    "\n",
    "print(f\"Total documents loaded: {len(all_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained RAG model and tokenizer\n",
    "rag_tokenizer = RagTokenizer.from_pretrained(os.getenv('RAG_MODEL_NAME'))\n",
    "rag_model = RagSequenceForGeneration.from_pretrained(os.getenv('RAG_MODEL_NAME')).to(device)\n",
    "\n",
    "# Initialize the retriever with the combined documents\n",
    "retriever = RagRetriever.from_pretrained(\n",
    "    os.getenv('RAG_MODEL_NAME'),\n",
    "    index_name=\"custom\",\n",
    "    passages=all_documents  \n",
    ")\n",
    "\n",
    "# Save the retriever for later use\n",
    "retriever.save_pretrained(\"retriever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=os.getenv('OUTPUT_DIR'),\n",
    "    evaluation_strategy=os.getenv('EVAL_STRATEGY'),\n",
    "    learning_rate=float(os.getenv('LEARNING_RATE')),\n",
    "    per_device_train_batch_size=int(os.getenv('TRAIN_BATCH_SIZE')),\n",
    "    per_device_eval_batch_size=int(os.getenv('EVAL_BATCH_SIZE')),\n",
    "    weight_decay=float(os.getenv('WEIGHT_DECAY')),\n",
    "    save_total_limit=int(os.getenv('SAVE_TOTAL_LIMIT')),\n",
    "    num_train_epochs=int(os.getenv('NUM_TRAIN_EPOCHS')),\n",
    "    predict_with_generate=bool(os.getenv('PREDICT_WITH_GENERATE')),\n",
    "    fp16=True,  # Enable mixed precision training if your GPU supports it\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "dataset = load_and_preprocess_dataset()\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=rag_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, tokenizer, dataset, device):\n",
    "    model.eval()\n",
    "    bleu = load_metric(\"bleu\")\n",
    "    rouge = load_metric(\"rouge\")\n",
    "    exact_matches = 0\n",
    "    total = len(dataset)\n",
    "\n",
    "    for data in tqdm(dataset, desc=\"Evaluating\"):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(input_ids=input_ids, max_length=50, num_beams=5)\n",
    "        output = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        reference = data['question']\n",
    "\n",
    "        # Compare generated output with reference\n",
    "        bleu.add(prediction=output.split(), reference=[reference.split()])\n",
    "        rouge.add(prediction=output, reference=reference)\n",
    "\n",
    "        # Exact match calculation\n",
    "        if output.strip() == reference.strip():\n",
    "            exact_matches += 1\n",
    "\n",
    "    bleu_score = bleu.compute()\n",
    "    rouge_score = rouge.compute()\n",
    "    exact_match_score = exact_matches / total\n",
    "\n",
    "    print(f\"BLEU Score: {bleu_score['bleu']}\")\n",
    "    print(f\"ROUGE Score: {rouge_score}\")\n",
    "    print(f\"Exact Match Score: {exact_match_score}\")\n",
    "\n",
    "# Load the dataset and tokenizer\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your validation/test dataset here\n",
    "validation_dataset = load_and_preprocess_dataset(dataset_name=\"squad\", split=\"validation[:10%]\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(rag_model, tokenizer, validation_dataset, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example questions for testing\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who is the president of the United States?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    # Tokenize the input question\n",
    "    input_ids = rag_tokenizer(question, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Generate the answer using the RAG model\n",
    "    generated = rag_model.generate(input_ids=input_ids, max_length=50, num_beams=5)\n",
    "    output = rag_tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Generated Answer: {output}\\n\")\n",
    "\n",
    "    # Retrieve documents (optional)\n",
    "    retrieved_docs = retriever.retrieve(input_ids=input_ids)\n",
    "    print(f\"Retrieved Documents: {retrieved_docs}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
